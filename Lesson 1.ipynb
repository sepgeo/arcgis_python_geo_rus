{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Зададим директорию с результатами анализов\n",
    "folder_path = r'C:\\Projects\\geochem_python_lessons\\Lessons\\1 Load tables\\Analytics_Actlabs'\n",
    "\n",
    "# Найдём все xlsx таблицы в директории\n",
    "files_list = glob.glob(folder_path + '\\*.xlsx')\n",
    "print(*files_list, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем функцию-шаблон для чтения таблицы с результатами аналищов\n",
    "def read_excel(filepath):\n",
    "    # читаем xlsx таблицу\n",
    "    report_df = pd.read_excel(filepath, sheet_name = 'Results', header = None)\n",
    "    \n",
    "    # читаем номер и дату отчёта в отдельные переменные\n",
    "    report_number = report_df.iloc[0, 0].split(': ')[1]\n",
    "    report_date_str = report_df.iloc[1, 0].split(': ')[1]\n",
    "    report_date = datetime.datetime.strptime(report_date_str, '%d/%m/%Y')\n",
    "    \n",
    "    # получим метаданные\n",
    "    elements_metadata = report_df.iloc[2:6, :]\n",
    "    elements_metadata = elements_metadata.set_index(0)\n",
    "    elements_metadata.index.name = None\n",
    "    full_names = []\n",
    "    for col in elements_metadata:\n",
    "        name = elements_metadata.loc['Analyte Symbol', col] + '_' + \\\n",
    "                elements_metadata.loc['Unit Symbol', col].replace('%', 'pct') + '_' + \\\n",
    "                elements_metadata.loc['Analysis Method', col].replace('-','_')\n",
    "        full_names.append(name)\n",
    "    elements_metadata.columns = full_names\n",
    "    elements_metadata\n",
    "    \n",
    "    # прочитаем результаты анализов\n",
    "    df = report_df.iloc[6:].copy()\n",
    "    df.columns = ['SampleID'] + full_names\n",
    "    df['Report_Number'] = report_number\n",
    "    df['Report_Date'] = report_date\n",
    "    \n",
    "    return elements_metadata, df\n",
    "\n",
    "# прочитаем все таблицы и добавим в список\n",
    "tables_list = []\n",
    "for file in files_list:\n",
    "    elements_metadata, file_df = read_excel(file)\n",
    "    tables_list.append(file_df)\n",
    "\n",
    "# объединим таблицы в сводную\n",
    "df = pd.concat(tables_list, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Найдём дубликаты\n",
    "all_ids = list(df['SampleID'])\n",
    "duplicate_ids = [i for i in set(all_ids) if all_ids.count(i) > 1]\n",
    "\n",
    "# исключим дубликаты\n",
    "df = df.sort_values(by = 'Report_Date').drop_duplicates(subset = 'SampleID', keep = 'last').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим наличие дубликатов\n",
    "print(df.loc[df['SampleID'].isin(duplicate_ids), ['SampleID', 'Au_ppb_AR_MS', 'Report_Date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прочитаем таблицу soil_samples\n",
    "points_link = r'C:\\Projects\\geochem_python_lessons\\Lessons\\1 Load tables\\Lesson_1.gdb\\soil_samples'\n",
    "fields  = ['SampleID', 'X_coord', 'Y_coord']\n",
    "points_list = [i for i in arcpy.da.SearchCursor(points_link, fields)]\n",
    "points_df = pd.DataFrame(points_list, columns = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим координаты и результаты анализов\n",
    "df['SampleID'] = df['SampleID'].apply(lambda x: x.upper())\n",
    "df_merged = pd.merge(points_df, df, on = 'SampleID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# найдём все текстовые значения по столбцам\n",
    "def show_string_values(columns, df):\n",
    "    all_string_list = []\n",
    "    for col in columns:\n",
    "        index = df[col].apply(lambda x: isinstance(x, str))\n",
    "        string_values_list = df.loc[index, col].unique()\n",
    "        if len(string_values_list) > 0:\n",
    "            string = col + ': ' + ', '.join(string_values_list)\n",
    "            all_string_list.append(string)\n",
    "    if len(all_string_list) > 0:\n",
    "        print(*all_string_list, sep = '\\n')\n",
    "    else:\n",
    "        print('Текстовые значения не найдены')\n",
    "\n",
    "show_string_values(elements_metadata, df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для замены текстовых значений\n",
    "def replace_text_value(value):\n",
    "    # определим тип значения\n",
    "    if any([type(value) == int,\n",
    "           type(value) == float,\n",
    "           value is None]):\n",
    "        return value\n",
    "    \n",
    "    # если значение не числовое, то попробуем найти номер\n",
    "    pattern = re.compile(r'[<>]?\\s*(\\d+(\\.\\d+)?)')\n",
    "    match = pattern.search(value)\n",
    "    if bool(match):\n",
    "        number = float(match.group(1))\n",
    "        if '<' in value:\n",
    "            number /= 2\n",
    "        elif '>' in value:\n",
    "            number *= 1.1\n",
    "        return number\n",
    "    elif bool(re.search(r'n.*a', value, re.IGNORECASE)):\n",
    "        return None\n",
    "\n",
    "# выполним замену текстовых значений\n",
    "for col in elements_metadata:\n",
    "    df_merged[col] = df_merged[col].apply(lambda x: replace_text_value(x))\n",
    "    \n",
    "show_string_values(elements_metadata, df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним таблицу метаданных\n",
    "for col in elements_metadata:\n",
    "    # вычислим количество числовых и NaN значений\n",
    "    num_idx = df_merged[col].notna()\n",
    "    numeric_count = len(df_merged.loc[num_idx, col])\n",
    "    nan_count = len(df_merged.loc[~num_idx, col])\n",
    "    elements_metadata.loc['N', col] = numeric_count\n",
    "    elements_metadata.loc['N/A', col] = nan_count\n",
    "\n",
    "    # вычислим значения меньше предела обнаружения\n",
    "    dl = elements_metadata.loc['Detection Limit', col]\n",
    "    less_dl_idx = df_merged[col] < dl\n",
    "    less_dl_count = len(df_merged.loc[less_dl_idx, col])\n",
    "    elements_metadata.loc['Less than D.L.', col] = less_dl_count\n",
    "\n",
    "    # посчитаем количество значений равное пределу обнаружения\n",
    "    equal_dl_idx = df_merged[col] == dl\n",
    "    equal_dl_count = len(df_merged.loc[equal_dl_idx, col])\n",
    "    elements_metadata.loc['Equal D.L.', col] = equal_dl_count\n",
    "\n",
    "    # вычислим коэффициент чувствительности\n",
    "    sensitivity_k = (less_dl_count + equal_dl_count/2) / numeric_count\n",
    "    elements_metadata.loc['Sensitivity k', col] = sensitivity_k\n",
    "\n",
    "    # базовая описательная статистика\n",
    "    mean_value = df_merged[col].mean()\n",
    "    std_value = df_merged[col].std()\n",
    "    mean_geom_value = np.log10(df_merged[col]).mean()\n",
    "    log10_std_value = np.log10(df_merged[col]).std()\n",
    "    elements_metadata.loc['Mean value', col] = mean_value\n",
    "    elements_metadata.loc['STD value', col] = std_value\n",
    "    elements_metadata.loc['Mean geom value', col] = mean_geom_value\n",
    "    elements_metadata.loc['STD log10 value', col] = log10_std_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим путь для экспорта таблиц\n",
    "folder_path = r'C:\\Projects\\geochem_python_lessons\\Lessons\\1 Load tables\\DATA'\n",
    "file_name = 'final_table.xlsx'\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# запишем две таблицы в один файл, но разные листы\n",
    "with pd.ExcelWriter(file_path) as writer:\n",
    "    df_merged.to_excel(writer, sheet_name = 'Results', index=False)\n",
    "    elements_metadata.to_excel(writer, sheet_name = 'Metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация в ArcGIS\n",
    "# Вариант 1\n",
    "\n",
    "# настроим окружение\n",
    "arcpy.env.workspace = r\"C:\\Projects\\geochem_python_lessons\\Lessons\\1 Load tables\\Lesson_1.gdb\"\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# импортируем таблицу excel\n",
    "arcpy.conversion.ExcelToTable(file_path, 'Lab_results', 'Results')\n",
    "\n",
    "# изменим псевдонимы переменных таблицы\n",
    "for col in elements_metadata:\n",
    "    alias = col.split('_')[0] + ' (' + col.split('_')[1] + ')'\n",
    "    arcpy.management.AlterField('Lab_results', col, new_field_alias = alias)\n",
    "\n",
    "# сделаем копию класса с точками опробования\n",
    "arcpy.management.CopyFeatures('soil_samples', 'soil_samples_with_analysis_option_1')\n",
    "\n",
    "# создадим выборку столбцов для объединения\n",
    "soil_samples_fields = [i.name for i in arcpy.ListFields('soil_samples_with_analysis_option_1')]\n",
    "fields_to_join = [i for i in df_merged.columns if i not in soil_samples_fields]\n",
    "\n",
    "# сделаем постоянное объединение таблицы и точечного класса\n",
    "arcpy.management.JoinField(in_data = 'soil_samples_with_analysis_option_1', \n",
    "                           in_field = 'SampleID', \n",
    "                           join_table = 'Lab_results', \n",
    "                           join_field = 'SampleID', \n",
    "                           fields = fields_to_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2\n",
    "# Экспорт таблицы в Numpy array и экспорт его в класс данных\n",
    "\n",
    "# проверим типы данных (object -> string)\n",
    "df_update = df_merged.copy()\n",
    "for col in df_update:\n",
    "    if df_update[col].dtype == 'object':\n",
    "        df_update[col] = df_update[col].astype('|S')\n",
    "\n",
    "# конвертируем pandas DF в Numpy array. Последний в класс данных\n",
    "structured_array = df_update.to_records()\n",
    "sr = arcpy.Describe('soil_samples').spatialReference\n",
    "output_feature_class = r'C:\\Projects\\geochem_python_lessons\\Lessons\\1 Load tables\\Lesson_1.gdb\\soil_samples_with_analysis_option_2'\n",
    "arcpy.da.NumPyArrayToFeatureClass(structured_array, output_feature_class, ('X_coord', 'Y_coord'), sr)\n",
    "arcpy.management.MakeFeatureLayer(output_feature_class, 'soil_samples_with_analysis_option_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 3\n",
    "# Обновим напрямую класс данных\n",
    "\n",
    "# сделаем копию класса данных\n",
    "output_fc = 'soil_samples_with_analysis_option_3'\n",
    "arcpy.management.CopyFeatures('soil_samples', output_fc)\n",
    "\n",
    "# сделаем выборку полей\n",
    "soil_samples_fields = [i.name for i in arcpy.ListFields('soil_samples_with_analysis_option_3')]\n",
    "fields_to_join = [i for i in df_merged.columns if i not in soil_samples_fields]\n",
    "\n",
    "# Добавим поля\n",
    "for col in fields_to_join:\n",
    "    # подготовим пседоним\n",
    "    alias = col.split('_')[0] + ' (' + col.split('_')[1] + ')'\n",
    "    \n",
    "    # подготовим тип данных\n",
    "    df_type = df_merged[col].dtype\n",
    "    if df_type == 'object':\n",
    "        field_type = 'TEXT'\n",
    "    elif df_type == 'float64':\n",
    "        field_type = 'FLOAT'\n",
    "    elif df_type == 'int64':\n",
    "        field_type = 'LONG'\n",
    "    elif df_type == '<M8[ns]':\n",
    "        field_type = 'DATE'\n",
    "    \n",
    "    # добавим поле\n",
    "    arcpy.management.AddField(in_table = output_fc,\n",
    "                             field_name = col,\n",
    "                             field_type = field_type,\n",
    "                             field_alias = alias)    \n",
    "    \n",
    "# обновим значения\n",
    "df_update = df_merged.set_index('SampleID')\n",
    "for idx, item in df_update.loc[:, fields_to_join].iterrows():\n",
    "    with arcpy.da.UpdateCursor(output_fc, fields_to_join, f\"SampleID = '{idx}'\") as cur:\n",
    "        for row in cur:\n",
    "            cur.updateRow(list(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
